{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import tables as tb\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sys\n",
    "import tempfile\n",
    "import argparse\n",
    "import scipy.io\n",
    "import pylab as p\n",
    "import matplotlib.cm as cm\n",
    "import struct\n",
    "\n",
    "from h5py import Dataset\n",
    "from six import string_types\n",
    "from six.moves import zip\n",
    "\n",
    "import phy\n",
    "from phy.io import KwikModel\n",
    "from phy.io import create_kwik\n",
    "from phy.utils._misc import _read_python\n",
    "from phy.utils.logging import debug\n",
    "from phy.utils.settings import _load_default_settings, _ensure_dir_exists\n",
    "from attrdict import AttrDict\n",
    "from phy.scripts.phy_script import _create_session\n",
    "from phy.traces import Filter, Thresholder, compute_threshold, FloodFillDetector, WaveformExtractor, PCA\n",
    "from phy.detect.spikedetekt import SpikeDetekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Folder = '/media/matias/DATA/WORKSPACE2/'\n",
    "\n",
    "matfile = 'FC-151217-7_waves.mat'\n",
    "\n",
    "\n",
    "channelnumber = '2'   ### or 1 depending where we start coubn t\n",
    "waveformstart = 14\n",
    "waveformend = 78\n",
    "paramsfile='params.prm'\n",
    "overwrite = True\n",
    "kwik_path = 'data' + channelnumber +'.kwik'\n",
    "clustfile = 'data.clu.'+ channelnumber\n",
    "n_channels = 1\n",
    "n_features = 3\n",
    "group = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_matdata(matfile, channelnumber, waveformstart, waveformend) :\n",
    "    #This concatenates the waveforms and makes fake continuous data to be put into spikedetekt\n",
    "    matdata = scipy.io.loadmat(matfile)\n",
    "    \n",
    "    #Find how many spikes there were on the channel to preallocate memory for waveforms and save time\n",
    "    totalspikes = 0\n",
    "    for key in matdata.keys() :\n",
    "        if key.find('Wspk')!=-1 :\n",
    "            if key.rsplit('Wspk', 1)[1][0:-3]==channelnumber :\n",
    "                totalspikes += matdata[key].shape[1]\n",
    "\n",
    "    wave_raw = np.zeros([totalspikes, waveformend-waveformstart])\n",
    "    spike_indices = np.zeros([totalspikes])\n",
    "    metadata = {}\n",
    "    i=0\n",
    "    for key in matdata.keys() :\n",
    "        if key.find('Wspk')!=-1 :\n",
    "            if key.rsplit('Wspk', 1)[1][0:-3]==channelnumber :\n",
    "                for j in np.arange(matdata[key].shape[1]) :\n",
    "                    metadata[i] = {}\n",
    "                    metadata[i]['episode'] = key.rsplit('Wspk', 1)[0][2:]\n",
    "                    metadata[i]['channel'] = key.rsplit('Wspk', 1)[1][0:-3]\n",
    "                    keyVspk = 'Ep'+metadata[i]['episode']+'Vspk'+metadata[i]['channel']+'Vu1'\n",
    "                    metadata[i]['spiketime'] = matdata[keyVspk][j,0]\n",
    "                    spike_indices[i] = matdata[keyVspk][j,0]\n",
    "                    metadata[i]['episode_spikenumber'] = j\n",
    "                    wave_raw[i,:] = matdata[key][waveformstart:waveformend,j]\n",
    "                    i+=1\n",
    "        \n",
    "    cont_data = np.reshape(wave_raw, [np.size(wave_raw)])\n",
    "    cont_data = cont_data.astype(np.int32)\n",
    "    \n",
    "    f = open('data.dat','wb')\n",
    "    data = bytearray()\n",
    "    for i in np.arange(len(cont_data)) :\n",
    "        data.extend(struct.pack('l', cont_data[i]))\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "\n",
    "    return wave_raw, spike_indices, metadata\n",
    "\n",
    "def write_spikestimes(filename, spike_times):\n",
    "    data = spike_times\n",
    "    np.save(filename, data)\n",
    "\n",
    "def KK2_prepare_textfiles(kwik_path, n_features, channelnumber):\n",
    "    filename, file_extension = op.splitext(kwik_path)\n",
    "    model = KwikModel(kwik_path)\n",
    "    features = model.features[:, :]\n",
    "    times = model.spike_times\n",
    "    times = np.expand_dims(times, axis=1)\n",
    "    \n",
    "    # make the text files to go into klustakwik2, fet.0 file and fmasks.0 file\n",
    "    numfeatures = n_features+1\n",
    "    data = np.concatenate((features, times), axis=1)\n",
    "\n",
    "    np.savetxt(filename + '.fet.' + channelnumber, data, header = str(numfeatures), fmt = '%10.5f', comments='', delimiter=' ')\n",
    "\n",
    "    dataMASKS = np.ones(data.shape)\n",
    "    np.savetxt(filename + '.fmask.' + channelnumber, dataMASKS, header = str(numfeatures), fmt = '%10.5f', comments='', delimiter=' ')\n",
    "\n",
    "def add_clusters(clustfile, session, group):\n",
    "    \n",
    "    clusters = np.loadtxt(clustfile, dtype='int64', unpack=False)\n",
    "    numclusters = clusters[0]+1\n",
    "    clusters = clusters[1::]\n",
    "    \n",
    "    sc = clusters.astype(np.int32)\n",
    "    session.model.creator.add_clustering(group=group, name='main', spike_clusters=sc)\n",
    "    session.emit('open')\n",
    "        \n",
    "def initialize_session(paramsfile, overwrite, kwikfile):\n",
    "    args = AttrDict({'file': paramsfile, 'overwrite': overwrite, 'kwikpath': kwikfile})\n",
    "    params = _read_python(args.file)\n",
    "\n",
    "    kwik_path = create_kwik(args.file, overwrite=args.overwrite, kwik_path=args.kwikpath)\n",
    "    interval = None\n",
    "\n",
    "    args.file = args.kwikpath\n",
    "    session = _create_session(args, use_store=False)\n",
    "\n",
    "    return dict(session=session, interval=interval)\n",
    "\n",
    "def run_faux_detection(session):\n",
    "    \n",
    "    sd_dir = op.join(session.settings.exp_settings_dir, 'spikedetekt')\n",
    "    _ensure_dir_exists(sd_dir)\n",
    "    interval_samples = None\n",
    "    traces = session.model.traces\n",
    "    params = session.model.metadata\n",
    "    params['probe_channels'] = session.model.probe.channels_per_group\n",
    "    params['probe_adjacency_list'] = session.model.probe.adjacency\n",
    "    debug(\"Running SpikeDetekt with the following parameters: \" \"{}.\".format(params))\n",
    "    \n",
    "    sd = SpikeDetekt(tempdir=sd_dir, **params)\n",
    "    out = sd.run_serial(traces, interval_samples=interval_samples)\n",
    "\n",
    "    return sd, out, params\n",
    "\n",
    "def replace_spikes(session, sd, out, wave_raw, group, n_channels, n_features):\n",
    "    out.spike_samples[0] = np.linspace(0, wave_raw.shape[1]*wave_raw.shape[0], wave_raw.shape[0]+1, dtype='int')+16\n",
    "    out.spike_samples[0] = out.spike_samples[0][0:-1] \n",
    "    spike_samples = out.spike_samples[0]\n",
    "    n_spikes = len(spike_samples) if spike_samples is not None else 0\n",
    "    spike_recordings = None\n",
    "    out.masks[0] = np.ones([len(spike_samples),1], dtype='float32')\n",
    "    wave_raw = np.expand_dims(wave_raw, axis=2)\n",
    "    pcs = sd.waveform_pcs(wave_raw, out.masks[0])\n",
    "    out.features[0] = sd.features(wave_raw, pcs)\n",
    "    \n",
    "    session.model.creator.add_spikes(group=group,\n",
    "                                spike_samples=spike_samples,\n",
    "                                spike_recordings=None,\n",
    "                                masks=out.masks[0],\n",
    "                                features=out.features[0],\n",
    "                                n_channels=n_channels,\n",
    "                                n_features=n_features,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:45:46 [I] Saving a backup of the Kwik file in c:\\Spike_Sorting\\Spikes_sorted\\OLD\\June-18\\WaveForms_Ch32\\data5.kwik.bak.\n",
      "10:45:46 [I] Finding the thresholds...\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "wave_raw, spike_indices, metadata = process_matdata(matfile, channelnumber, waveformstart, waveformend)\n",
    "write_spikestimes('spike_times.npy', spike_indices/np.float(30000))\n",
    "wave_raw = wave_raw.astype(np.float32)\n",
    "newsession = initialize_session(paramsfile, overwrite, kwik_path)\n",
    "sd, out, params = run_faux_detection(newsession['session'])\n",
    "replace_spikes(newsession['session'], sd, out, wave_raw, group, n_channels, n_features)\n",
    "KK2_prepare_textfiles(kwik_path, n_features, channelnumber)\n",
    "%run C:\\Miniconda3\\envs\\phy\\Scripts\\kk2_legacy-script.py data $channelnumber max_possible_clusters=6 num_starting_clusters=3 max_iterations=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_clusters(clustfile, newsession['session'], group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wave_raw, spike_indices, metadata = process_matdata(matfile, channelnumber, waveformstart, waveformend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = np.loadtxt('data.clu.5', dtype='int64', unpack=False)\n",
    "clusters = clusters[1::]\n",
    "pcs = np.loadtxt('data.fet.'+channelnumber, dtype='float32', unpack=False, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcs1 = pcs[np.where(clusters==1)[0],:]\n",
    "pcs3 = pcs[np.where(clusters==3)[0],:]\n",
    "pcs4 = pcs[np.where(clusters==4)[0],:]\n",
    "pcs5 = pcs[np.where(clusters==5)[0],:]\n",
    "pcs6 = pcs[np.where(clusters==6)[0],:]\n",
    "\n",
    "waves1 = wave_raw[np.where(clusters==1)[0],:]\n",
    "waves3 = wave_raw[np.where(clusters==3)[0],:]\n",
    "waves4 = wave_raw[np.where(clusters==4)[0],:]\n",
    "waves5 = wave_raw[np.where(clusters==5)[0],:]\n",
    "waves6 = wave_raw[np.where(clusters==6)[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(pcs1[:,0], pcs1[:,1], pcs1[:,2], color = 'b')\n",
    "ax.scatter(pcs3[:,0], pcs3[:,1], pcs3[:,2], color = 'r')\n",
    "ax.scatter(pcs4[:,0], pcs4[:,1], pcs4[:,2], color = 'g')\n",
    "ax.scatter(pcs5[:,0], pcs5[:,1], pcs5[:,2], color = 'k')\n",
    "ax.scatter(pcs6[:,0], pcs6[:,1], pcs6[:,2], color = 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in np.arange(waves1.shape[0]) :\n",
    "    plt.plot(waves1[i,:], 'b')\n",
    "#for i in np.arange(waves3.shape[0]) :\n",
    "    #plt.plot(waves3[i,:], 'r')\n",
    "#for i in np.arange(500) :\n",
    "    #plt.plot(waves4[i,:], 'g')\n",
    "#for i in np.arange(500) :\n",
    "    #plt.plot(waves6[i,:], 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
